<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.0">Jekyll</generator><link href="https://pyomin.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://pyomin.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-28T18:18:15+00:00</updated><id>https://pyomin.github.io/feed.xml</id><title type="html">Pyo Min Hong</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Target Aware No-code neural network Generation and Operation framework</title><link href="https://pyomin.github.io/blog/2024/ETRI/" rel="alternate" type="text/html" title="Target Aware No-code neural network Generation and Operation framework"/><published>2024-12-24T11:52:16+00:00</published><updated>2024-12-24T11:52:16+00:00</updated><id>https://pyomin.github.io/blog/2024/ETRI</id><content type="html" xml:base="https://pyomin.github.io/blog/2024/ETRI/"><![CDATA[<blockquote> <p><strong>Announcement</strong></p> <ul> <li><a href="https://github.com/ML-TANGO/TANGO/releases/tag/tango-24.11">2024 November TANGO Release</a></li> </ul> </blockquote> <hr/> <h2 id="introduction-to-tango-">Introduction to TANGO <a name="intro"></a></h2> <p>TANGO (<strong>T</strong>arget <strong>A</strong>ware <strong>N</strong>o-code neural network <strong>G</strong>eneration and <strong>O</strong>peration framework) is code name of project for Integrated Machine Learning Framework.</p> <p>The TANGO framework is an automated neural network generation and deployment solution designed for novice users with little to no coding expertise. It simplifies the process of creating and deploying neural network applications by providing an intuitive environment, including a project manager and neural network visualization tools. Users are required only to prepare labeled datasets and define target devices. TANGO performs an analysis of the datasets and device characteristics, generates task-specific neural networks aligned with user requirements, trains the models, and packages them as Docker container images for seamless deployment onto the specified target devices.</p> <p>Each component of TANGO is a self-contained service implemented using container technology, interacting with other components through REST APIs. The entire framework can be accessed and executed via <a href="https://github.com/ML-TANGO/TANGO">TANGO GitHub repository</a>.</p> <p>To ensure users understand the neural network architecture, TANGO provides a visual representation of the base model. Our team developed the visualization tools and implemented the code deployment framework to support this functionality. This post provides a brief overview of the implemented features.</p> <p align="center"> <img src="../../../assets/img/Projects/Velcro_0.png" alt="Velcro Viz0" width="800px"/> </p> <h3 id="a-layer-tab">A. Layer Tab</h3> <p>The <strong>Layer Tab</strong> represents the types of layers that can be employed when constructing the deep learning architecture.</p> <ul> <li><strong>Types of layers</strong>: Set based on the official <strong>PyTorch</strong> documentation and represented as a toggle.</li> <li><strong>Layers</strong>: Become visible when pressing the type toggle and are color-coded according to their respective types.</li> </ul> <h3 id="b-abstract-tab">B. Abstract Tab</h3> <p>The <strong>Abstract Tab</strong> represents a feature to group the architecture configured by the user and its functionality is divided into auto-group and custom-group.</p> <ul> <li><strong>Auto-Group</strong>: Automatically groups nodes and edges according to pre-defined levels (e.g., 3 levels in ResNet50) based on key architectural characteristics.</li> <li><strong>Custom-Group</strong>: Allows users to manually group selected nodes and edges into specific grouping levels based on user-defined criteria.</li> </ul> <h3 id="c-layer-information">C. Layer Information</h3> <p>The <strong>Layer Information</strong> represents a feature to edit the hyperparameters of the layers that consist of the architecture.</p> <h3 id="d-workspace">D. Workspace</h3> <p>The <strong>Workspace</strong> is a space for configuring architecture using the layers from the Layer Tab, and it offers functionalities such as Alignment, Architecture Validation, and Deep Learning Code Generation.</p> <ul> <li><strong>Alignment</strong>: Vertically aligns the nodes in the workspace to provide a clear and organized view of the architecture.</li> <li><strong>Architecture Validation</strong>: Displays the results of dimension compatibility validation between nodes in red and parameter variability validation in blue.</li> <li><strong>Deep Learning Code Generation</strong>: Exports the user-configured architecture as a code file in a format supported by PyTorch (i.e., pth).</li> </ul> <p align="center"> <img src="../../../assets/img/Projects/Velcro_use1.png" alt="Velcro Viz1" width="800px"/> </p> <h3 id="a-how-to-construct-architecture">A. How to construct architecture</h3> <ul> <li><strong>Add Node</strong>: By dragging a layer node from the Layer Selection to Workspace, the desired node is created in the Workspace.</li> <li><strong>Connect Nodes</strong>: By dragging the top or bottom points of a node to the points of another node, the edge is created between the two nodes.</li> <li><strong>Eliminate Node or Edge</strong>: By clicking the target node or edge and pressing the backspace key, the target node is eliminated.</li> </ul> <h3 id="b-how-to-set-hyper-parameter">B. How to set hyper-parameter</h3> <ul> <li>By clicking the node in the workspace, you can see the layer information window located at the bottom. Then you can edit the value of hyper-parameters in each field.</li> <li><strong>Default button</strong>: By clicking the default button located at the bottom-left of the layer information window, the value of hyper-parameters will be changed to the default value.</li> <li><strong>Save button</strong>: By clicking the save button located at the bottom-right of the layer information window, the value of hyper-parameters will be saved to the database.</li> </ul> <h3 id="c-how-to-scale--align">C. How to scale &amp; align</h3> <ul> <li><strong>Scaling up &amp; down</strong>: By scrolling the mouse wheel upward, nodes and edges will zoom in for a magnified view. Conversely, by scrolling the mouse wheel downward, nodes and edges will zoom out to show a reduced size.</li> <li><strong>Architecture Alignment</strong>: By clicking the Alignment button located in the bottom left of the Workspace, the nodes in the Workspace are aligned vertically.</li> </ul> <h3 id="d-how-to-validate-architecture--generate-code">D. How to validate architecture &amp; generate code</h3> <ul> <li><strong>Architecture Validation</strong>: By clicking the Inspect button located in the bottom right of the Workspace, the results of dimension compatibility between nodes are displayed in red, and parameter variability is displayed in blue.</li> <li><strong>Deep Learning Code Generation</strong>: By clicking the Generate button located in the bottom right of the Workspace, a code file format (i.e., pth) of the constructed architecture is exported.</li> </ul> <p><img src="../../../assets/img/Projects/Velcro_Group.png" alt="Velcro Viz2" width="800px"/></p> <h3 id="e-how-to-use-auto-grouping">E. How to use auto-grouping</h3> <ul> <li>By clicking the level buttons, the user can group layers for the desired level.</li> <li>The information about abstracted layers can be found in the group information window.</li> <li><strong>Levels</strong> <ul> <li>The higher the level, the more layers can be abstracted into groups.</li> <li>For example, in the case of VGG-16, it can be grouped as follows. <ul> <li>Level 1: None of the nodes are represented as a group.</li> <li>Level 2: The “Conv2d”, “BatchNorm2d”, and “ReLU” nodes are represented as a group.</li> <li>Level 3: The “Conv2d”, “BatchNorm2d”, “ReLU”, “Conv2d”, “BatchNorm2d”, “ReLU”, and “MaxPool2d” are represented as one group. And “Conv2d”, “BatchNorm2d”, “ReLU”, “Conv2d”, “BatchNorm2d”, “ReLU”, “Conv2d”, “BatchNorm2d”, “ReLU”, “MaxPool2d” is represented by another group.</li> </ul> </li> </ul> </li> </ul> <h3 id="f-how-to-use-custom-grouping">F. How to use custom-grouping</h3> <ul> <li><strong>How to group</strong> <ul> <li>Select nodes in the workspace and click the ‘Group’ button, the visualized architecture in the workspace automatically groups the chosen nodes and edges into the corresponding grouping level.</li> </ul> </li> <li><strong>How to ungroup</strong> <ul> <li>Click the ‘Ungroup’ button in the Abstract Architecture Tab, and then the group will be ungrouped.</li> </ul> </li> </ul> <p><br/></p> <h3 id="key-features-of-this-implementation">Key features of this implementation:</h3> <ul> <li> <p>Developing a visual-based programming tool to enable seamless construction, modification, analysis, and refinement of deep learning architectures</p> </li> <li> <p>Supporting developers of all skill levels by converting user-configured architectures into target deep learning code for direct deployment</p> </li> <li> <p>Implementing as a web application and deploying using Docker, with confirmed compatibility for popular deep learning architectures and frameworks</p> <p><br/> <br/></p> </li> </ul> <hr/> <h2 id="acknowledgement-">Acknowledgement <a name="ack"></a></h2> <p>This work was supported by <a href="https://www.iitp.kr/">Institute of Information &amp; communications Technology Planning &amp; Evaluation (IITP)</a> grant funded by the Korea government(MSIT) (<strong>No. 2021-0-00766</strong>, <em>Development of Integrated Development Framework that supports Automatic Neural Network Generation and Deployment optimized for Runtime Environment</em>).</p>]]></content><author><name></name></author><category term="Project"/><category term="ETRI"/><summary type="html"><![CDATA[Supported by Institute for Information communication Technology Planning (IITP), South Korea]]></summary></entry></feed>